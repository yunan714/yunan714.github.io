<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Neural_Network_Initialization]]></title>
    <url>%2F2019%2F07%2F19%2FNeural-Network-Initialization%2F</url>
    <content type="text"><![CDATA[# 简介对于神经网络这种复杂非凸问题来说，初始化是非常重要的，对于不同的场景应当选择适合的初始化方法。以下介绍常用的初始化方法。 0初始化对于convex optimization问题来说，0初始化便可以达到很好的效果，但是这对于神经网络来说却是一种极为不利的方法，如果使用0初始化，则对应的每个神经元的参数都将是一致的，那么每次通过损失值梯度回传的更新参数也都是一致的，高度对称的权重使得神经网络失去了其最为重要的自动发掘特征的能力。 随机初始化目前最为常用的是随机初始化，但是针对不同问题需要选择合适的分布，不然同样会影响网络的性能。 假定我们有一个以tanh为激活函数的神经网络，对于其参数我们使用均值为0，方差为0.01的高斯分布。创建一个10层的神经网络，则每一层的输出值分布直方图如下： 实现代码： 123456789101112131415161718192021222324252627282930313233343536373839import tensorflow as tfimport numpy as npimport matplotlib.pyplot as plt%matplotlib inlinegraph = tf.Graph()with graph.as_default(): data = tf.constant(np.random.randn(2000, 800).astype('float32')) layer_sizes = [800 - 50 * i for i in range(0,10)] num_layers = len(layer_sizes) fcs = [] for i in range(0, num_layers - 1): X = data if i == 0 else fcs[i - 1] node_in = layer_sizes[i] node_out = layer_sizes[i + 1] W = tf.Variable(np.random.randn(node_in, node_out).astype('float32'))/np.sqrt(node_in) #W = tf.Variable(np.random.randn(node_in, node_out).astype('float32'))# * 0.01 # W = tf.Variable(np.random.randn(node_in, node_out).astype('float32')) fc = tf.matmul(X, W)# fc = tf.contrib.layers.batch_norm(fc, center=True, scale=True,# is_training=True) fc = tf.nn.tanh(fc) fcs.append(fc) with tf.Session(graph=graph) as sess: sess.run(tf.global_variables_initializer()) print('input mean &#123;0:.5f&#125; and std &#123;1:.5f&#125;'.format(np.mean(data.eval()), np.std(data.eval()))) for idx, fc in enumerate(fcs): print('layer &#123;0&#125; mean &#123;1:.5f&#125; and std &#123;2:.5f&#125;'.format(idx+1, np.mean(fc.eval()), np.std(fc.eval()))) plt.figure(figsize=(18,6)) for idx, fc in enumerate(fcs): plt.subplot(1, len(fcs), idx+1) plt.hist(fc.eval().flatten(), 30, range=[-1,1]) 随着层数的增加，很明显可以查看出输出值迅速向0靠拢，后面的层输出几乎全是0，通过反向传播得到的梯度也就变成了0，使得神经网络难以继续优化下去。 反之，若是我们将方差调大，变成1，输出如下： 为了解决这个问题，Glorot和Bengio在2010提出了Xavier初始化方法。 Xavier Initialization对于神经网络，我们希望他的输入空间和输出空间是同一个分布（一般用方差来衡量）。如果输入空间过于稀疏而输出空间过于稠密，那么通过反向传播得到的损失函数所返回的梯度就会变得很小使得网络难以训练；反之则会导致梯度爆炸。 在神经网络中层与层之间的传递主要是线性组合的形式：$$Z = \sum_{i=1}^{n}w_ix_i$$方差$$Var(w_ix_i)=Var(w_i)Var(x_i)+E^2(w_i)Var(x_i)+E^2(x_i)Var(w_i)$$ 推导 已知$Var(x)=E(x^2)-E^2(x)$$$Var(xy)=E(x^2y^2)-E^2(xy)$$由于$x$与$y$相互独立$$\begin{split}Var(xy)&amp;=E(x^2)E(y^2)-E^2(x)E^2(y)\&amp;=(Var(x)+E^2(x))(Var(y)+E^2(y))-E^2(x)E^2(y)\&amp;=Var(x)Var(y)+E^2(x)Var(y)+E^2(y)Var(x)\end{split}$$ 通过假定$E(w_i)$与$E(x_i)$均为0（这个可以通过batch normalization对每一层重新调整分布做到，事实上影响不大），我们有$$Var(w_ix_i)=Var(w_i)Var(x_i)$$因为假定$w_i与x_i$均服从独立同分布假设（所有机器学习的通用假设），$Var(x_i)=Var(x)$，最终：$$Var(z)=nVar(x)Var(w)$$要保持输入$x$与输出的$z$方差不变，使$w$得方差为1即可。从正向和反向两个角度来看，$n_{in}$与$n_{out}$是不同的，此处对其取平均值，得到$w$方差表达式：$$Var(w)=\frac{2}{n_{in}+n_{out}}$$假定$w$为均匀分布，均匀分布对应的方差为$$var(w)=\frac{(a-b)^2}{12}$$推出$w$的分布为:$$w\sim U[-\sqrt{\frac{6}{n_{in}+n_{out}}},\sqrt{\frac{6}{n_{in}+n_{out}}}]$$这便是Xavier初始化方法。 同样，我们使用代码对其进行验证： 可以看出效果非常的好，即使是在第十层也维持了良好的分布。 但是Xavier是在线性变换的基础上提出来的，上述的实验也是建立在激活函数为tanh的情况下，理论上来说Xavier对于非线性激活函数是不具有普适性的，那么我们尝试将激活函数替换为RELU再次进行实验： 果然，一开始效果还可以，后来就基本全部接近于0了。 为了解决Relu激活函数的初始化问题，Kaiming He大佬与2015年提出了一种新的初始化方法，可以帮助激活函数为Relu的神经网络获得良好的初始化，称之为He初始化。 He Initialization]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>初始化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MCMC-1-Markov-Chain]]></title>
    <url>%2F2019%2F07%2F19%2FMCMC-2-Markov-Chain%2F</url>
    <content type="text"><![CDATA[马尔科夫链概述马尔科夫链假设每一个时间步$t$的状态$X_t$的状态仅依赖于上一个时间步的状态$X_{t-1}$。用公式表达就是：$$P\left(X_{t+1} | \ldots X_{t-2}, X_{t-1}, X_{t}\right)=P\left(X_{t+1} | X_{t}\right)$$对于一个马尔科夫链来说，只要我们能够求出任意两个状态之间的转换概率，那么随着马尔科夫链的不断迭代，其最终状态的概率必定收敛。（前提是各转移概率不为0，也就是说每个状态都可以转换为任意状态。） 举一个列子： 上图是一个股票市场的马尔科夫链，共有三种状态，牛市（Bull market）, 熊市（Bear market）和横盘（Stagnant market） ，定义牛市为0，熊市为1，横盘为2，矩阵$P_{ij}$表示从状态$i$转移到状态$j$的概率。最终我们获得马尔科夫状态转移矩阵如下：$$P=\left(\begin{array}{ccc}{0.9} &amp; {0.075} &amp; {0.025} \ {0.15} &amp; {0.8} &amp; {0.05} \ {0.25} &amp; {0.25} &amp; {0.5}\end{array}\right)$$接下来我们使用代码验证其必定收敛的性质： 12 模型状态概率最终会收敛于[ 0.625 0.3125 0.0625]，而且事实上无论你怎么初始化状态概率，模型最终都会收敛到这个状态。 同时，对于一个确定的状态转移矩阵$P$，它的n次幂$P^n$对应的状态概率也会收敛到同样的稳定分布。 基于马尔科夫链进行采样如果我们能够得到一个分布对应的马尔科夫状态转移矩阵，我们就能采样出这个平稳分布的样本集。 任意选择一个初始分布$\pi_0(x)$，如高斯分布，对其进行采样获得样本$x_0$，基于状态转移条件概率$P(x|x_0)$采样状态值$x_1$，如此重复，当达到一定次数$n_1$时，我们便可以假定分布已经收敛到平稳分布，再重复$n_2$次，获得的$(x_{n_1}，x_{n_1+1},…x_{n_1+n_2})$即可视作我们的样本集，$n_2$为样本集中样本的数目。 小结使用马尔科夫链进行采样存在一个问题，如何获得转移概率矩阵$P$呢？详见下一节。 说明本文内容主要来自刘建平老师的博客https://www.cnblogs.com/pinard/p/6632399.html]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>统计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nlp task metric]]></title>
    <url>%2F2019%2F07%2F19%2Fnlp-task-metric%2F</url>
    <content type="text"><![CDATA[BELUbelu是机器翻译领域的指标，公式如下：$$B L E U=B P \cdot \exp \left(\sum_{n=1}^{N} w_{n} \log P_{n}\right)$$ $$B P=\left{\begin{array}{ll}{1} &amp; {\text { if } c&gt;r} \ {e^{1-r / c}} &amp; {\text { if } c \leq r}\end{array}\right.$$ 这个指标具体是如何运算的呢？ 列子： 候选译文（Predicted）：It is a guide to action which ensures that the military always obeys the commands of the party 参考译文（Gold Standard）1：It is a guide to action that ensures that the military will forever heed Party commands2：It is the guiding principle which guarantees the military forces always being under the command of the Party3：It is the practical guide for the army always to heed the directions of the party 首先计算$P_n$,若$n=4$，我们需要从$1gram$一直计算到$4gram$。$ngram$的计算方法是，对于候选译文中每个出现的n元组，统计其次数。对于每个三元组，定义max值为候选译文和参考译文中的最大值，定义min为候选译文和参考译文（除去最小的那个）的最小值。 最后将每个n元组的min值相加之和除以每个n元组的候选译文出现个数相加，得到的值即为$P_n$。 $w_i$为对应的$ngram$的权重，将所有$P_n$加权求和再取指数，最后乘上一个长度惩罚系数$BP$即可得到最终的belu值。 $BP$系数会惩罚过短的句子，它会自动得寻找尽可能跟标准翻译长度相近似的句子。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>评价指标</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mcmc:(1) 蒙特卡洛]]></title>
    <url>%2F2019%2F07%2F19%2FMCMC-1-Monte-Carlo-Method%2F</url>
    <content type="text"><![CDATA[MCMC——part1:蒙特卡洛搜索MCMC概述MCMC(Markov Chain Monte Carlo 马尔科夫链蒙特卡洛) 蒙特卡洛方法蒙特卡洛方法最早被用于求解一些不好计算的积分问题，如下图： 为了解决这个问题，我们假定使用一个值$f(x)$来代表区间$[a,b]$上所有值，此时积分的近似解就为:$$(b-a)f(x)$$使用一个值估计当然是不合理的，但是我们可以使用多个值来估计，假定我们对区间$[a,b]$进行n次采样，得到的采样$x_i$对应的被积函数值为$f(x_i)$，积分的近似解即为：$$\frac{b-a}{n} \sum_{i=0}^{n-1} f\left(x_{i}\right)$$但是这种情况是建立在$x$是均匀分布的情况下，而事实上大多数问题中的”$x$”都不会是服从均匀分布的。假定我们得到了$x$的概率分布函数$P(x)$，我们便可以求得更加准确的积分：$$\theta = \int_a^b f(x)dx = \int_a^b \frac{f(x)}{p(x)}p(x)dx\approx\frac{1}{n}\sum^{n-1}_{i=0}\frac{f(x_i)}{p(x_i)}$$ 基于概率分布进行采样假定我们已知$x$的概率分布，要使用Monte Carlo对$f(x)$积分进行估计，需要依据概率分布$p(x)$对$x$进行采样。对于常见的分布如正态分布、t分布等是可以很容易的通过已有的库获得的。但是对于一些不常见的分布，如何对其进行采样呢？ 接受-拒绝采样假定一个可以采样的分布，如高斯分布$q(x)$，称为proposal distribution。 设定一个常量$k$使得$p(x)$总在$kq(x)$的下方，如上图。 首先采样得到$q(x)$的一个样本$z_0$，然后从均匀分布$(0,kq(z_0))$中采样得到一个值$u$，如果$u$落在上方灰色区域则拒绝此次采样，否则接受此次采样，重复直到获得足够数目的样本。 试着分析一下这种做法，每次采样获得的样本接受概率如下:$$p(\text {accept})=\int \frac{\tilde{p}(z)}{k q(z)} q(z) d z=\frac{1}{k} \int \tilde{p}(z) d z$$下面我们尝试使用拒绝接受采样对分布$$\tilde{p}(z)=0.3 \exp \left(-(z-0.3)^{2}\right)+0.7 \exp \left(-(z-2)^{2} / 0.3\right)$$进行采样。归一化常数为1.2113，参考分布为$Gussian(1.4,1.2)$。 python代码实现 1234567891011121314151617181920212223242526import matplotlib.pyplot as pltimport numpy as np#绘制目标分布def cal_px(x): return (0.3*np.exp(-(x-0.3)**2) + 0.7* np.exp(-(x-2.)**2/0.3))/1.2113x = np.arange(-4.,6.,0.01)plt.plot(x,cal_px(x),color='r')#生成qz的分布size = int(1e+07)sigma = 1.2z = np.random.normal(loc=1.4,scale=sigma,size=size)qz = 1/(np.sqrt(2*np.pi)*sigma)*np.exp(-0.5*(z-1.4)**2/sigma**2)k = 2.5#再0到kq(z)之间均匀采样u = np.random.uniform(low = 0, high = k*qz, size = size)#从sample中选出符合条件的zpz = 0.3*np.exp(-(z-0.3)**2) + 0.7* np.exp(-(z-2.)**2/0.3)sample = z[pz &gt;= u]#绘制采样的图像plt.hist(sample,bins=150, normed=True, edgecolor='black')plt.show() 结果如下，可以看出效果还是非常好的： 一些问题 对于高维分布要找到合适的$q(x)$和$k$是非常困难的。 对于二维分布，一般得到的是其条件分布而非一般形式的$p(x,y)$，这时接受拒绝是无法使用的。 转载说明本文主要内容来自刘建平老师的文章，原文链接https://www.cnblogs.com/pinard/p/6625739.html]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>统计</tag>
        <tag>蒙特卡洛</tag>
      </tags>
  </entry>
</search>
