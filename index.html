<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-文本摘要" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/07/18/文本摘要/" class="article-date">
  <time datetime="2019-07-18T12:26:16.000Z" itemprop="datePublished">2019-07-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/07/18/文本摘要/">文本摘要</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="文本摘要"><a href="#文本摘要" class="headerlink" title="文本摘要"></a>文本摘要</h1><ul>
<li>单文档 &amp; 多文档（此处仅仅关注单文档）</li>
<li>抽取式 &amp; 生成式</li>
<li>有监督 &amp; 无监督</li>
</ul>
<h2 id="抽取式"><a href="#抽取式" class="headerlink" title="抽取式"></a>抽取式</h2><blockquote>
<p>从原文中抽取出<strong>关键词、关键句</strong>形成摘要。 </p>
</blockquote>
<h3 id="1-传统方法"><a href="#1-传统方法" class="headerlink" title="1.传统方法"></a>1.传统方法</h3><ul>
<li>Lead3：直接抽取文章前三句作为摘要。emmmmmmmmmmmm</li>
<li>TextRank：将句子作为节点，根据句子<strong>相似度</strong>来构造<strong>无向有权边</strong>，迭代更新权值，最终选择得分最高的N个节点对应的句子作为摘要</li>
<li>聚类：设法得到句子向量表示，通过聚类算法获得N个簇，选择每个簇中最靠近中心的结合起来形成摘要。</li>
</ul>
<h3 id="2-神经网络（序列标注）"><a href="#2-神经网络（序列标注）" class="headerlink" title="2.神经网络（序列标注）"></a>2.神经网络（序列标注）</h3><blockquote>
<p>为每个句子进行二分类，判断其是否可作为摘要。将正类结合形成摘要</p>
</blockquote>
<ul>
<li>《Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. In Thirty-First AAAI Conference on Artificial Intelligence, 2017.  》</li>
</ul>
<p>本文提出了<strong>Summarunner</strong>，模型结构如下：$\color{blue}{没找到开源代码}$</p>
<p><img src="images/SummaRuNNer.png" alt="Summarunner"></p>
<p>[1] 将word级别的词向量输入进双向GRU网络中获得句向量表示</p>
<p>[2] 将所有句向量输入进双向GRU网络中</p>
<p>[3] 将[2]中输出的所有向量通过averaged pooling获得文档的向量表示<br>$$<br>\mathrm{d}=\tanh \left(W_{d} \frac{1}{N_{d}} \sum_{j=1}^{N^{d}}\left[\mathbf{h}<em>{j}^{f}, \mathbf{h}</em>{j}^{b}\right]+\mathbf{b}\right)<br>$$<br><em>将每个sentence的正向与反向输出向量拼接起来进行平均池化，再映射回原来的维度。</em></p>
<p>[4]针对每一个句子，通过上一个句子的判断结果与文档向量以及本句的输出向量联合起来输出本句结果。<br>$$<br>\begin{array}{r}{P\left(y_{j}=1 | \mathbf{h}<em>{j},\mathbf{s}</em>{j},\mathbf{d}\right)=\sigma\left(W_{c} \mathbf{h}<em>{j}\right.} {+\mathbf{h}</em>{j}^{T} W_{s} \mathbf{d}}  {-\mathbf{h}<em>{j}^{T} W</em>{r} \tanh \left(\mathbf{s}<em>{\mathbf{j}}\right)}  {+W</em>{a p \mathbf{p}<em>{j}^{\prime}}}  {+W</em>{r p \mathbf{p}_{j}^{r}}}  {+b )}\end{array}<br>$$</p>
<ul>
<li>$W_ch_j$表示第$j$句的信息</li>
<li>$h_j^TW_sd$表示句子和文档之间的salience（显著性）</li>
<li>$h_j^TW_rtank(s_j)$捕获了<strong>句子对于当前摘要的冗余</strong></li>
<li>而最后两个$W_{apP_j^a}$和$W_{rpP_j^r}$则用于句子对于文章相对位置和绝对位置的影响。将$P^r$和$P^a$作为模型参数自行学习。$\color{red}{位置向量这部分不是很懂。}$</li>
</ul>
<p>上式中$y_j$ 代表第$j$个句子是否属于摘要中，$h_j$和$d$分别是当前第$j$句的表示和文档向量表示，而此处的$s_j$则是摘要在第$j$个句子的位置的动态表示，由下式给出：<br>$$<br>\mathbf{s}<em>{j}=\sum</em>{i=1}^{j-1} \mathbf{h}<em>{i} P\left(y</em>{i}=1 | \mathbf{h}<em>{i}, \mathbf{s}</em>{i}, \mathbf{d}\right)<br>$$<br>当计算到第$j$个句子时，根据前面前$j-1$个句子中被选为摘要的概率将输出进行加权求和，作为summary的当前冗余向量，<strong>保证意义相近的句子不会被重复地纳入summary之中。</strong>$\color{red}{但是通过加权求和来计算当前冗余是否恰当？两者很有可能不在同一个尺度之下了。}$</p>
<p>最终模型的目标函数如下：<br>$$<br>\begin{aligned} l(\mathbf{W}, \mathbf{b}) &amp;=-\sum_{d=1}^{N} \sum_{j=1}^{N_{d}}\left(y_{j}^{d} \log P\left(y_{j}^{d}=1 | \mathbf{h}<em>{j}^{d}, \mathbf{s}</em>{j}^{d}, \mathbf{d}<em>{d}\right)\right.\ &amp;+\left(1-y</em>{j}^{d}\right) \log \left(1-P\left(y_{j}^{d}=1 | \mathbf{h}<em>{j}^{d}, \mathbf{s}</em>{j}^{d}, \mathbf{d}_{d}\right)\right) \end{aligned}<br>$$<br>使用了负对数似然（交叉熵）进行训练。</p>
<p><strong>Extractive Training</strong></p>
<p>关于文本摘要的语料的target一般都是由人撰写的抽象的表达句子，为了方便本文的训练，我们使用greed search，逐个搜索句子使得subset与ground truth的Rouge指标达到最大，获得训练所需的标签。</p>
<p><strong>Abstractive Training</strong></p>
<p>为了避免上述的举动，在双向GRU公式中引入了$s_{j-1}$的信息，并仅仅在训练时再Summarunner中的最后一步添加一个解码器decoder，用于解码出摘要信息。<br>$$<br>\begin{aligned} \mathbf{u}<em>{k} &amp;=\sigma\left(\mathbf{W}</em>{u x}^{\prime} \mathbf{x}<em>{k}+\mathbf{W}</em>{u h}^{\prime} \mathbf{h}<em>{k-1}+\mathbf{W}</em>{u c}^{\prime} \mathbf{s}<em>{-1}+\mathbf{b}</em>{u}^{\prime}\right) \ \mathbf{r}<em>{k} &amp;=\sigma\left(\mathbf{W}</em>{r x}^{\prime} \mathbf{x}<em>{k}+\mathbf{W}</em>{r h}^{\prime} \mathbf{h}<em>{k-1}+\mathbf{W}</em>{r c}^{\prime} \mathbf{s}<em>{-1}+\mathbf{b}</em>{r}^{\prime}\right) \ \mathbf{h}<em>{k}^{\prime} &amp;=\tanh \left(\mathbf{W}</em>{h x}^{\prime} \mathbf{x}<em>{k}+\mathbf{W}</em>{h h}^{\prime}\left(\mathbf{r}<em>{k} \odot \mathbf{h}</em>{k-1}\right)+\right. \mathbf{W}<em>{h c}^{\prime} \mathbf{S}</em>{-1}+\mathbf{b}<em>{h}^{\prime} ) \end{aligned}<br>$$<br>解码器<br>$$<br>\begin{aligned} \mathbf{f}</em>{k} &amp;=\tanh \left(\mathbf{W}<em>{f h}^{\prime} \mathbf{h}</em>{k}+\mathbf{W}<em>{f x}^{\prime} \mathbf{x}</em>{k}+\mathbf{W}<em>{f c}^{\prime} \mathbf{s}</em>{-1}+\mathbf{b}<em>{f}^{\prime}\right) \ \mathbf{P}</em>{\mathbf{v}}(\mathbf{w})<em>{k} &amp;=\operatorname{softmax}\left(\mathbf{W}</em>{v}^{\prime} \mathbf{f}<em>{k}+\mathbf{b}</em>{v}^{\prime}\right) \end{aligned}<br>$$<br>$\color{red}{此处解码器的长度是怎么定的？是否有终止token?}$</p>
<p>目标函数<br>$$<br>l\left(\mathbf{W}, \mathbf{b}, \mathbf{W}^{\prime}, \mathbf{b}^{\prime}\right)=-\sum_{k=1}^{N_{s}} \log \left(\mathbf{P}<em>{\mathbf{v}}\left(w</em>{k}\right)\right)<br>$$</p>
<p>直觉上来说，由于文摘表示$s_{j-1}$是作为SummaRuNNer与Decoder之间唯一的信息通道，因此最大化decoder解码出的abstractive summary会迫使模型去学习一个尽可能好的表达来获得更加准确的抽取概率$P(y_j)$估计。</p>
<hr>
<blockquote>
<p>序列标注结合seq2seq和强化学习</p>
<p>$\color{blue}{无开源代码}$</p>
</blockquote>
<ul>
<li>Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative adversarial nets with policy gradient. In Thirty-First AAAI Conference on Artificial Intelligence, 2017. Xingxing Zhang, Mirella Lapata, Furu Wei, and Ming Zhou. Neural latent extractive document summarization. arXiv preprint arXiv:1808.07187, 2018. </li>
</ul>
<blockquote>
<p>seq2seq方法</p>
</blockquote>
<ul>
<li><p>Pointer-Generator: 于ACL2017提出，是一种经典的文本摘要抽取框架$\color{blue}{开源代码}$</p>
<p><strong>Baseline-seq2seq</strong></p>
<p><img src="images/baseline_seq2seq.png" alt></p>
<p>seq2seq由编码器与解码器两部分组成。通过编码器将序列信息编码输出为一个隐藏状态$h_i$，然后通过解码器将其解码为$s_t$，同时在解码每一个时间步$t$时使用注意力机制来获取当前的上下文状态$h_t^<em>$。<br>$$<br>\begin{aligned} e_{i}^{t} &amp;=v^{T} \tanh \left(W_{h} h_{i}+W_{s} s_{t}+b_{\mathrm{attn}}\right) \ a^{t} &amp;=\operatorname{softmax}\left(e^{t}\right)<br>\h_{t}^{</em>}&amp;=\sum_{i} a_{i}^{t} h_{i}<br>\end{aligned}<br>$$<br>在解码段的每个时间步，通过当前状态$s_t$与上下文状态$h_t^<em>$通过两层线性层映射为词表分布$P_{vocab}$:<br>$$<br>P_{\text { vocab }}=\operatorname{softmax}\left(V^{\prime}\left(V\left[s_{t}, h_{t}^{</em>}\right]+b\right)+b^{\prime}\right)<br>$$<br>这样便得到了词表上的softmax概率分布，进而可以预测生成的词。在第$t$步的损失函数为$loss_t=-log P(w_t^<em>)$，此时原输入序列的整体损失为：<br>$$<br>\operatorname{loss}=\frac{1}{T} \sum_{t=0}^{T} \operatorname{loss}_{t}<br>$$<br>*</em>Pointer-Generator Networks**</p>
<p>在传统的seq2seq模型上增加了pointer-network的copy机制，对于每个词判断是要从原文中copy还是从登入词中根据概率生成，解决了原文中的未登录词问题。文章引入了一个权重$P_{Gen}$来判断词是生成的还是复制的。</p>
<p><img src="images/pointer-generator.png" alt></p>
<p>通过在Baseline-seq2seq中得到的$s_t$和$h_t^<em>$，以及解码器端的输入$x_t$一起计算$P_{Gen}$：<br>$$<br>P_{\mathrm{gen}}=\sigma\left(w_{h^{</em>}}^{T} h_{t}^{*}+w_{s}^{T} s_{t}+w_{x}^{T} x_{t}+b_{\mathrm{ptr}}\right)<br>$$<br>这样会扩充本来的单词表形成一个更大的单词表，预测概率变为:<br>$$<br>P(w)=p_{\mathrm{gen}} P_{\mathrm{vocab}}(w)+\left(1-p_{\mathrm{gen}}\right) \sum_{i : w_{i}=w} a_{i}^{t}<br>$$<br>其中$\alpha_i^t$表示原文档中的词。我们可以看到解码器一个词同时考虑了其输入生成还是拷贝的可能。当一个词不出现在常规词表中其概率$P_{vocab}$为0，反之该词不出现在文档中时$\sum_{i : w_{i}=w} a_{i}^{t}$为0。</p>
<p>$\color{red}{这样的概率限制实际上是如何实现的呢？}$</p>
<p><strong>Coverage Mechanism</strong></p>
<p>将先前的时间步的注意力权重加到一起得到覆盖向量$c^t$，用先前的注意力权重来影响当前的注意力权重，避免生成重复的文本。<br>$$<br>c^{t}=\sum_{t^{\prime}=0}^{t-1} a^{t^{\prime}}\<br>e_{i}^{t}=v^{T} \tanh \left(W_{h} h_{i}+W_{s} s_{t}+\color{red}{w_{c} c_{i}^{t}}+b_{\mathrm{attn}}\right)<br>$$<br>同时注意为coverage vector添加损失函数:<br>$$<br>cvtloss_{t}=\sum_{i} \min \left(a_{i}^{t}, c_{i}^{t}\right)<br>$$<br>这样保证了$cvtloss_{t} \leq \Sigma_{i} a_{i}^{t}=1$是一个有界的量，最终loss为:<br>$$<br>\operatorname{loss}<em>{t}=-\log P\left(w</em>{t}^{*}\right)+\lambda \sum \min \left(a_{i}^{t}, c_{i}^{t}\right)<br>$$<br>分析一下，coverage loss为了使损失值最小，目的是使得$\sum c_i^t \leq 1$,这样保证了对于每个时间步$t$，其对应的覆盖向量$c^t$始终与其他状态表示维持在统一尺度上，不会影响计算。</p>
<hr>
</li>
</ul>
<ul>
<li><p>Aishwarya Jadhav and Vaibhav Rajan. Extractive summarization with swap-net: Sentences and words from alternating pointer networks. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 142–151, 2018. </p>
<p>$\color{blue}{开源代码}$</p>
<p>SWAP-NET：同时使用Alternating Pointer Networks从原文中抽取Sentence和Words。</p>
<p><strong>问题描述</strong></p>
<p>$D$ ：输入文档</p>
<p>$s_1,s_2,…,s_N$：文档中的$N$个句子</p>
<p>$w_1,w_2,…,w_n$:文档中的$n$个词 $\color{red}{这个词是按顺序忽视句子边界还是word集合？}$</p>
<p><em>本文使用words和sentence之间的交互来预测重要的word和sentence。</em>定义一个由重要句子和单词的indices组成的target sequence $V=v_1,…,v_m$，里面的每个一个index $v_j$都可以指向原文中的一个句子或者一个单词。基于此设计了一个端到端的seq2seq模型可以同时预测salient sentence 和key word。</p>
<p><strong>模型结构</strong></p>
<p><img src="images/SWAP-net.png" alt></p>
</li>
</ul>
<p><strong>Encoder</strong></p>
<p>使用了两个encoder，一个word级的bi-lstm和一个sentence级的lstm，sentence的向量等于对应的sentence输入word级bi-lstm中的最后一个输出。</p>
<p><strong>Decoder</strong></p>
<p>两个decoder,均为lstm，一个是指向句子的pointer-network一个是指向单词的pointer-network，定义$T_m$为sentence-level decoder所生成的句子Index序列，同理$t_m$为word-level。</p>
<p><strong>Network Details</strong></p>
<p>在第$j$个decoding步，我们需要通过$Q_j$来判断是选择一个句子还是一个单词。定义$\alpha_{kj}^s$为在第$j$步选择第$k$个sentence的概率，同理有$\alpha_{kj}^w$为在第$j$步选择第$k$个word的概率。<br>$$<br>\alpha_{k j}^{s}=p\left(T_{j}=k | v_{&lt;j}, Q_{j}=1, D\right)\<br>\alpha_{i j}^{w}=p\left(t_{j}=i | v_{&lt;j}, Q_{j}=0, D\right)<br>$$</p>
<p>$$<br>v_{j}=\left{\begin{array}{ll}{k=\arg \max <em>{k} p</em>{k j}^{s}} &amp; {\text { if } \max <em>{k} p</em>{k j}^{s}&gt;\max <em>{i} p</em>{i j}^{w}} \ {i=\arg \max <em>{i} p</em>{i j}^{w}} &amp; {\text { if } \max <em>{i} p</em>{i j}^{w}&gt;\max <em>{k} p</em>{k j}^{s}}\end{array}\right.<br>$$</p>
<p>$$<br>\begin{array}{c}{\alpha_{i j}^{w}=\operatorname{softmax}\left(v_{t}^{T} \phi\left(w_{h} h_{j}+w_{t} e_{i}\right)\right)} \ {\alpha_{k j}^{s}=\operatorname{softmax}\left(V_{T}^{T} \phi\left(W_{H} H_{j}+W_{T} E_{k}\right)\right)}\end{array}<br>$$</p>
<p>$$<br>\begin{array}{l}{h_{j}=L S T M\left(h_{j-1}, a_{j-1}, \phi\left(A_{j-1}\right)\right)} \ {H_{j}=L S T M\left(H_{j-1}, A_{j-1}, \phi\left(a_{j-1}\right)\right)}\end{array}<br>$$</p>
<p>上式$h_j$和$H_j$分别是在第$j$个decoding步里word-level和sentence-level的隐藏层。其中的$\phi$表示非线性映射，此处为tanh。其中$a_{j}=\sum_{i=0}^{n} \alpha_{i j}^{w} e_{i}, A_{j}=\sum_{k=0}^{N} \alpha_{k j}^{s} E_{k}$。$e_i$与$E_k$分别代表word-level与sentence-level的encoder层的隐藏层向量。</p>
<p>switch probability $p\left(Q_{j} | v_{&lt;j}, D\right)$(第j个解码步选择词或者句子的概率)：<br>$$<br>\begin{array}{l}{p\left(Q_{j}=1 | v_{&lt;j}, D\right)=}  {\sigma\left(w_{Q}^{T}\left(H_{j-1}, A_{j-1}, \phi\left(h_{j-1}, a_{j-1}\right)\right)\right)} \ {p\left(Q_{j}=0 | v_{&lt;j}, D\right)=1-p\left(Q_{j}=1 | v_{&lt;j}, D\right)}\end{array}<br>$$<br>loss function at step j:<br>$$<br>l_{j}=-\log \left(p_{k j}^{s} q_{j}^{s}+p_{i j}^{w} q_{j}^{w}\right)- {\log p\left(Q_{j} | v_{&lt;j}, D\right) }<br>$$<br>在第$j$个step，如果是个单词，则$q_{j}^{w}=1, q_{j}^{s}=0$；反之是个句子，则$q_{j}^{w}=0, q_{j}^{s}=1$。</p>
<p>“we assign importance scores to the selected sentences based on their probability values during decoding as well as the probabilities of the selected words that are present in the selected sentence”</p>
<p>基于在解码期间句子对应的概率值以及句子中含有的单词的概率值我们将句子的重要性重新分配。</p>
<p>这样包含被解码器选中的句子会获得更高的importance。</p>
<p>假定第$k$个input sentence $s_k$ 在第$j$步被选中，第$i$个input word $w_i$ 在第$l$步被选中，这样$s_k$的importance被定义为:<br>$$<br>I\left(s_{k}\right)=\alpha_{k j}^{s}+\lambda \sum_{w_{i} \in s_{k}} \alpha_{i l}^{w}<br>$$<br>$\color{red}{这里的l是怎么和k,j联系起来的？关系没有描述清楚。}$</p>
<p>$\color{red}{最后就是通过公式转换importance后单独选择句子？原文中并未描述清楚，只有看了代码后才知道！}$</p>
<blockquote>
<p>句子排序方法，对每个句子输出概率，从中抽取topK句子作为摘要</p>
</blockquote>
<ul>
<li><ul>
<li><input checked disabled type="checkbox"> Qingyu Zhou, Nan Yang, Furu Wei, Shaohan Huang, Ming Zhou, and Tiejun Zhao. Neural document summarization by jointly learning to score and select sentences. arXiv preprint arXiv:1807.02305, 2018. </li>
</ul>
</li>
<li>$\color{blue}{开源代码}$</li>
</ul>
<p>提出jointly学习句子选择和打分两个过程，在CNN/Daily Mail上取得state of art。</p>
<h2 id="生成式摘要"><a href="#生成式摘要" class="headerlink" title="生成式摘要"></a>生成式摘要</h2><blockquote>
<p>生成式摘要主要依托于seq2seq方式生成摘要，易出现未登录词和生成重复问题。</p>
</blockquote>
<hr>
<ul>
<li><ul>
<li><input checked disabled type="checkbox"> Abigail See, Peter J Liu, and Christopher D Manning. Get to the point: Summarization with pointer-generator networks. arXiv preprint arXiv:1704.04368, 2017. </li>
</ul>
<p><strong>于前文中已经介绍</strong></p>
</li>
</ul>
<hr>
<ul>
<li><ul>
<li><input checked disabled type="checkbox"> Romain Paulus, Caiming Xiong, and Richard Socher. A Deep Reinforced Model for Abstractive Summarization. CoRR, 2017. </li>
</ul>
<p>效果屌差,很无聊的一篇文章</p>
</li>
</ul>
<hr>
<ul>
<li><ul>
<li><input checked disabled type="checkbox"> Wei Li, Xinyan Xiao, Yajuan Lyu, and Yuanzhuo Wang. Improving neural abstractive document summarization with structural regularization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4078–4087, 2018c. </li>
</ul>
</li>
</ul>
<p>$\color{red}{网页打不开}$</p>
<hr>
<h3 id="1-引入外部信息"><a href="#1-引入外部信息" class="headerlink" title="1.引入外部信息"></a>1.引入外部信息</h3><hr>
<ul>
<li><input checked disabled type="checkbox"> Ziqiang Cao, Wenjie Li, Sujian Li, and Furu Wei. Retrieve, rerank and rewrite: <strong>Soft template</strong> based neural summarization. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 152–161, 2018. ~~</li>
</ul>
<p>$\color{red}{不想看}$</p>
<hr>
<h3 id="2-多任务学习"><a href="#2-多任务学习" class="headerlink" title="2.多任务学习"></a>2.多任务学习</h3><hr>
<ul>
<li><input checked disabled type="checkbox"> ~Han Guo, Ramakanth Pasunuru, and Mohit Bansal. Soft layer-specific multi-task summarization with entailment and question generation. arXiv preprint arXiv:1805.11004, 2018. ~</li>
</ul>
<p>$\color{red}{不想看}$</p>
<hr>
<h3 id="3-生成对抗"><a href="#3-生成对抗" class="headerlink" title="3.生成对抗"></a>3.生成对抗</h3><hr>
<ul>
<li><input disabled type="checkbox"> <p>Linqing Liu, Yao Lu, Min Yang, Qiang Qu, Jia Zhu, and Hongyan Li. <strong>Generative adversarial network for abstractive text summarization</strong>. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018. </p>
<p>$\color{red}{没有code}$</p>
</li>
</ul>
<p>与传统GAN区别不大，G是一个由文本到summary的net,D是一个CNN判别器，唯一需要注意的是两者都经过预训练。</p>
<p>还有一点不同之处在于生成器的损失函数中引入了policy gradient</p>
<hr>
<ul>
<li><input disabled type="checkbox"> <p>Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. <strong>Seqgan: Sequence generative adversarial nets with policy gradient</strong>. In Thirty-First AAAI Conference on Artificial Intelligence, 2017. Xingxing Zhang, Mirella Lapata, Furu Wei, and Ming Zhou. Neural latent extractive document summarization. arXiv preprint arXiv:1808.07187, 2018. </p>
<p>$\color{blue}{<a href="https://github.com/Lantao" target="_blank" rel="noopener">https://github.com/Lantao</a> Yu/SeqGAN}$</p>
</li>
</ul>
<hr>
<h3 id="4-抽取生成式"><a href="#4-抽取生成式" class="headerlink" title="4.抽取生成式"></a>4.抽取生成式</h3><hr>
<ul>
<li><input disabled type="checkbox"> Sebastian Gehrmann, Yuntian Deng, and Alexander M Rush. <strong>Bottom-Up Abstractive Summarization</strong>. EMNLP, 2018. </li>
</ul>
<hr>
<ul>
<li><input disabled type="checkbox"> Chenliang Li, Weiran Xu, Si Li, and Sheng Gao. <strong>Guiding generation for abstractive text summarization based on key information guide network</strong>. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), volume 2, pages 55–60, 2018a. </li>
</ul>
<hr>
<ul>
<li><input disabled type="checkbox"> Wei Li, Xinyan Xiao, Yajuan Lyu, and Yuanzhuo Wang. <strong>Improving neural abstractive document summarization with explicit information selection modeling</strong>. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1787–1796, 2018b. </li>
</ul>
<hr>
<h2 id="最新论文"><a href="#最新论文" class="headerlink" title="最新论文"></a>最新论文</h2><ul>
<li>Pretraining-Based Natural Language Generation for Text Summarization</li>
<li>Iterative Document Representation Learning Towards Summarization with Polishing</li>
<li>Deep Recurrent Generative Decoder for Abstractive Text Summarization</li>
<li>HIBERT:Document Level Pre_training of Hierarchical Bidirectional Transformers for Document Summarization</li>
</ul>
<h2 id="常用数据集"><a href="#常用数据集" class="headerlink" title="常用数据集"></a>常用数据集</h2><p>DUC、New York Times、CNN/Daily Mail、Gigaword、LCSTS</p>
<h2 id="额外重要概念补充介绍"><a href="#额外重要概念补充介绍" class="headerlink" title="额外重要概念补充介绍"></a>额外重要概念补充介绍</h2><h3 id="1-自动摘要评价指标"><a href="#1-自动摘要评价指标" class="headerlink" title="1.自动摘要评价指标"></a>1.自动摘要评价指标</h3><h4 id="Rouge-N"><a href="#Rouge-N" class="headerlink" title="Rouge-N"></a>Rouge-N</h4><p>$$<br>Rouge=\frac{两者之间共有的ngram数目}{参考摘要的ngram数目}<br>$$</p>
<p>因为在自动摘要领域我们更加关心的是召回Recall，所以分母一般是参考摘要。</p>
<h4 id="Rouge-L"><a href="#Rouge-L" class="headerlink" title="Rouge-L"></a>Rouge-L</h4><p>$$<br>\begin{aligned} R_{l c s} &amp;=\frac{L C S(X, Y)}{m} \ P_{l c s} &amp;=\frac{L C S(X, Y)}{n} \ F_{l c s} &amp;=\frac{\left(1+\beta^{2}\right) R_{l c s} P_{l c s}}{R_{l c s}+\beta^{2} P_{l c s}} \end{aligned}<br>$$</p>
<p>$LCS(X,Y)​$即为两者之间最长公共子串，$m,n​$分别为两个句子的长度，最终$Rouge-L​$表现为两者的compressive，但是在实际中$\beta​$一般取值很大，仍旧是主要考虑召回。</p>
<h4 id="Rouge-L的改进版-Rouge-W"><a href="#Rouge-L的改进版-Rouge-W" class="headerlink" title="Rouge-L的改进版 Rouge-W"></a>Rouge-L的改进版 Rouge-W</h4><h4 id="BELU"><a href="#BELU" class="headerlink" title="BELU"></a>BELU</h4><h3 id="2-exposure-bias-什么是暴露偏差"><a href="#2-exposure-bias-什么是暴露偏差" class="headerlink" title="2.exposure bias 什么是暴露偏差"></a>2.exposure bias 什么是暴露偏差</h3><h2 id="ACL2019论文"><a href="#ACL2019论文" class="headerlink" title="ACL2019论文"></a>ACL2019论文</h2><h3 id="x-Multi-News-a-Large-Scale-Multi-Document-Summarization-Dataset-and-Abstractive-Hierarchical-Model"><a href="#x-Multi-News-a-Large-Scale-Multi-Document-Summarization-Dataset-and-Abstractive-Hierarchical-Model" class="headerlink" title="- [x] Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model"></a>- [x] Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model</h3><p>$\color{red}{多文档类型}$</p>
<hr>
<h3 id="Improving-Abstractive-Document-Summarization-with-Salient-Information-Modeling"><a href="#Improving-Abstractive-Document-Summarization-with-Salient-Information-Modeling" class="headerlink" title="- [ ] Improving Abstractive Document Summarization with Salient Information Modeling"></a>- [ ] Improving Abstractive Document Summarization with Salient Information Modeling</h3><p>三层：</p>
<ul>
<li>encoding</li>
<li><strong>selective layer</strong></li>
<li>decoder</li>
</ul>
<p>![](E:\Learnings\自然语言处理方向\自动文摘\images\selective layer.png)</p>
<p>$\color{blue}{通过研究表明，人类在总结summary时\首先略过一篇文章，然后去掉不重要的部分。（Brown and Day 1983）}$</p>
<hr>
<h3 id="X-Inducing-Document-Structure-for-Aspect-based-Summarization"><a href="#X-Inducing-Document-Structure-for-Aspect-based-Summarization" class="headerlink" title="- [X] Inducing Document Structure for Aspect-based Summarization"></a>- [X] Inducing Document Structure for Aspect-based Summarization</h3><h3 id="x-BiSET-Bi-directional-Selective-Encoding-with-Template-for-Abstractive-Summarization"><a href="#x-BiSET-Bi-directional-Selective-Encoding-with-Template-for-Abstractive-Summarization" class="headerlink" title="- [x] BiSET: Bi-directional Selective Encoding with Template for Abstractive Summarization"></a>- [x] BiSET: Bi-directional Selective Encoding with Template for Abstractive Summarization</h3><blockquote>
<p>本文提出了一个模型，使用了从训练数据中获得的模板来来选择特定领域的关键信息来帮助引导摘要过程。</p>
<p><a href="https://github.com/InitialBug/BiSET" target="_blank" rel="noopener">https://github.com/InitialBug/BiSET</a></p>
</blockquote>
<p><strong>our models involves a novel bi-directional selective layer with two gates to mutually select key information from an article and its template to assist with summary generation.</strong></p>
<p><strong>我们还提出了一种通过多阶段过程自动从训练数据中提取高质量模板的方法。</strong></p>
<p>我们的模型分为三个部分：Retrieve, Fast Rerank, BiSET</p>
<p>step1:  对于每篇文章，Retrieve通过训练集返回少量候选模板</p>
<p>step2：Fast Rerank从候选模板中快速确定最好的模板</p>
<p>step3: BiSET从article和template中相互选择重要信息 去为summarization生成一个增强的文本表示</p>
<p><strong>1.Retrieve</strong></p>
<p>使用源文章查询训练语料库，找到几篇相关文章，使用它们的摘要（gold answer）作为候选模板。</p>
<p>$\color{red}{不是很懂这一步是如何实现的.下文中提到这一步主要使用简单的表面上的文字匹配来衡量文本的相似度。}$</p>
<p><strong>2.Fast Rerank</strong></p>
<p>作者提出传统的Rnn与Cnn并不适合于此处比较衡量文本和模板相似度，因而提出了自己model</p>
<p><strong>Convolutional Encoder Block</strong>：</p>
<p>![](C:\Users\hp\Desktop\papers\img\convolution encoder block.png)</p>
<p>给定embeddings${e_i}<em>{i=1}^E\in \mathcal{R}^d$，使用1-D卷积操作，卷积核为2d*kd，通过这个操作来获取n-gram  features：$h_i\in\mathcal{R}^{2d}$,之后只用gated linear unit(GLU)作为激活函数将其维度重新映射回d,$h</em>{i}=\left[h_{i}^{1} ; h_{i}^{2}\right], \text { where } h_{i}^{1}, h_{i}^{2} \in \mathbb{R}^{d}$，激活操作为$$r_{i}=G L U\left(h_{i}\right)=G L U\left(\left[h_{i}^{1} ; h_{i}^{2}\right]\right)=h_{i}^{1} \otimes \sigma\left(h_{i}^{2}\right)$$。</p>
<p>$\color{blue}{此处是将n个词对应的词向量映射为2d的形式，最后使用zero-padding使其维持原状。}$</p>
<p>同时为了保持信息使用residual block进行连接。</p>
<p><strong>Similarity Matrix</strong></p>
<p>之前的encoder为每个模板、文章生成高级的representation，$\mathbf{S} \in \mathbb{R}^{m \times d}$,$\mathbf{T} \in \mathbb{R}^{n \times d} $,$s_{i j}=f\left(\mathbf{S}<em>{i}, \mathbf{T}</em>{j}\right)$。$S$即为此处的矩阵。相似度函数$f$往往采用dot product或者bilinear function，而此处文章提出本文使用欧式距离反而能获得更好的效果<br>$$<br>f(x, y)=\exp \left(-|x-y|^{2}\right)<br>$$<br><strong>Pooling Layer</strong></p>
<p>这一步目的是过滤掉无关的信息，首先我们注意到在source article中存在着大量重复的words，但是我们只想计算一次，因此我们定义了salient weights：<br>$$<br>q=\max _{\operatorname{column}}(\mathcal{S})<br>$$<br>这里的max_column是一个列最大函数，选择出了（tamplate中的单词数量）个相似度后，使用k-max得到其中K个最大值，经过两层fm映射得到最终的score。</p>
<p>$\color{red}{理论上来说每个文章和模板的维度均不同，而训练神经网络则必须要固定尺度。可能采用clip or padding?感觉很奇怪}$</p>
<p>$\color{red}{还有一点，上文中行对应的article，使用列最大就保证了每个tamplate的词只对应一个原文中的\句子，避免了重复。但是这样有什么原理么，感觉是很直观的东西。可能只是为了快速的rerank。}$</p>
<p><strong>3 生成摘要</strong></p>
<p>此处先介绍了三种传统方法，共用一组编码解码器，只是interaction的部分不同</p>
<ul>
<li><p>Concatenation</p>
<p>将模板表达拼接在文章表达后。</p>
</li>
<li><p>Concatenation + Self-Attention</p>
<p>增加了multi-head self attention</p>
</li>
<li><p>DCN attention<create template-aware article representation></create></p>
<p>$s_{ij}=\mathbf{W}<em>{0}\left[h</em>{i}^{s} ; h_{j}^{t} ; h_{i}^{s} \otimes h_{j}^{t}\right]$</p>
<p>改变了相似度矩阵的计算方式</p>
<p>接着分别对相似度矩阵的行列使用softmax得到两个矩阵$\overline{\mathcal{S}} \text { and } \overline{\overline{\mathcal{S}}}$</p>
<p>article2template attention :$\mathbf{A}=\overline{\mathcal{S}} \cdot h^{t}$</p>
<p>template2article attention: $\mathbf{B}=\overline{\mathcal{S}} \cdot \overline{\overline{\mathcal{S}}}^T \cdot h^{s}$</p>
<p>最后获得tamplate-aware article representation : $z_{i}^{s}=\left[h_{i}^{s} ; \mathbf{A}<em>{i} ; h</em>{i}^{s} \otimes \mathbf{A}<em>{i} ; h</em>{i}^{s} \otimes \mathbf{B}_{i}\right]$</p>
</li>
</ul>
<p><strong>BiSET</strong></p>
<p>​    ![](C:\Users\hp\Desktop\papers\img\bi selective layer.png)</p>
<p>t2a 使用一个模板来帮助过滤文本信息<br>$$<br>\begin{array}{c}{g_{i}=\sigma\left(\mathbf{W}<em>{s h} h</em>{i}^{s}+\mathbf{W}<em>{t h} h^{t}+b</em>{s}\right)} \ {h_{i}^{g}=h_{i}^{s} \otimes g_{i}}\end{array}<br>$$<br>a2t被用来控制$h^g$在最终表达中的比例。<strong>因为我们不能保证模板的可信度。</strong>我们使用所有的文本资源最后的输出表达（正向最后一个输出和反向第一个输出的拼接）$h^s$和$h^t$来计算可信度等级$d$.<br>$$<br>d=\sigma\left(\left(h^{s}\right)^{T} \mathbf{W}<em>{d} h^{t}+b</em>{d}\right)<br>$$</p>
<p>$$<br>z_{i}^{s}=d h_{i}^{g}+(1-d) h_{i}^{s}<br>$$</p>
<p>最后加权求和得到了文章的最终表达。</p>
<p>$\color{red}{看起来好像是更加优秀的模板起到了帮助作用，但是实际上神经网络训练时通过目标gold answer也许就已经学到了一些模板吧？}$</p>
<p><img src="C:%5CUsers%5Chp%5CDesktop%5Cpapers%5Cimg%5Cdecoder.png" alt></p>
<p>解码器部分包含一个传统的Rnn：<br>$$<br>h_{t}^{c}=R N N\left(w_{t-1}, h_{t-1}^{c}\right)<br>$$<br><em>隐层使用原始文章表达$h^s$进行初始化，？？？？</em>$\color{red}{介是神恶魔意思}$<br>$$<br>\begin{array}{c}{\varepsilon_{t, i}=\left(z_{i}^{s}\right)^{T} \mathbf{W}<em>{c} h</em>{t}^{c}} \ {\alpha_{t, i}=\frac{\exp \left(\varepsilon_{t, i}\right)}{\sum_{i=1}^{M} \exp \left(\varepsilon_{t, i}\right)}} \ {c_{t}=\sum_{i=1}^{M} \alpha_{t, i} z_{i}^{s}}\end{array}<br>$$</p>
<p>$$<br>h_{t}^{a}=\tanh \left(\mathbf{W}<em>{h a}\left[c</em>{t} ; h_{t}^{c}\right]\right)\<br>p\left(w_{t} | w_{1}, \ldots, w_{t-1}\right)=\operatorname{softmax}\left(\mathbf{W}<em>{p} h</em>{t}^{a}\right)<br>$$</p>
<p>最后映射到词表上的概率。</p>
<p>在训练时：</p>
<p>rerank部分使用rouge计算metric，</p>
<p>$L_{r}(\theta)=-\frac{1}{N} \sum_{i=1}^{N}\left[s^{<em>} \log s+\left(1-s^{</em>}\right) \log (1-s)\right]$</p>
<p>BiSET</p>
<p>$L_{w}(\theta)=-\frac{1}{D} \sum_{i=1}^{D} \sum_{j=1}^{L} \log p\left(w_{j}^{*(i)} | w_{j-1}^{(i)}, x^{(i)}, y^{(i)}\right)$</p>
<hr>
<h3 id="x-Hierarchical-Transformers-for-Multi-Document-Summarization"><a href="#x-Hierarchical-Transformers-for-Multi-Document-Summarization" class="headerlink" title="- [x] Hierarchical Transformers for Multi-Document Summarization"></a>- [x] Hierarchical Transformers for Multi-Document Summarization</h3><p>  $\color{red}{多文档}$</p>
<hr>
<h3 id="X-Abstractive-text-summarization-based-on-deep-learning-and-semantic-content-generalization"><a href="#X-Abstractive-text-summarization-based-on-deep-learning-and-semantic-content-generalization" class="headerlink" title="- [X] Abstractive text summarization based on deep learning and semantic content generalization"></a>- [X] Abstractive text summarization based on deep learning and semantic content generalization</h3><p>这是本书………………</p>
<h3 id="X-A-Simple-Theoretical-Model-of-Importance-for-Summarization"><a href="#X-A-Simple-Theoretical-Model-of-Importance-for-Summarization" class="headerlink" title="- [X] A Simple Theoretical Model of Importance for Summarization"></a>- [X] A Simple Theoretical Model of Importance for Summarization</h3><h3 id="HighRES-Highlight-based-Reference-less-Evaluation-of-Summarization"><a href="#HighRES-Highlight-based-Reference-less-Evaluation-of-Summarization" class="headerlink" title="- [ ] HighRES: Highlight-based Reference-less Evaluation of Summarization"></a>- [ ] HighRES: Highlight-based Reference-less Evaluation of Summarization</h3><h3 id="HIBERT-Document-Level-Pre-training-of-Hierarchical-Bidirectional-Transformers-for-Document-Summarization"><a href="#HIBERT-Document-Level-Pre-training-of-Hierarchical-Bidirectional-Transformers-for-Document-Summarization" class="headerlink" title="- [ ] HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization"></a>- [ ] HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization</h3><h3 id="X-Scoring-Sentence-Singletons-and-Pairs-for-Abstractive-Summarization"><a href="#X-Scoring-Sentence-Singletons-and-Pairs-for-Abstractive-Summarization" class="headerlink" title="- [X] Scoring Sentence Singletons and Pairs for Abstractive Summarization"></a>- [X] Scoring Sentence Singletons and Pairs for Abstractive Summarization</h3><p>$\color{blue}{<a href="https://github.com/ucfnlp/summarization-sing-pair-mix}$" target="_blank" rel="noopener">https://github.com/ucfnlp/summarization-sing-pair-mix}$</a></p>
<hr>
<h3 id="x-Improving-the-Similarity-Measure-of-Determinantal-Point-Processes-for-Extractive-Multi-Document-Summarization"><a href="#x-Improving-the-Similarity-Measure-of-Determinantal-Point-Processes-for-Extractive-Multi-Document-Summarization" class="headerlink" title="- [x] Improving the Similarity Measure of Determinantal Point Processes for Extractive Multi-Document Summarization"></a>- [x] Improving the Similarity Measure of Determinantal Point Processes for Extractive Multi-Document Summarization</h3><p>$\color{red}{多文档}$</p>
<hr>
<h3 id><a href="#" class="headerlink" title=" "></a> </h3><h3 id="Searching-for-Effective-Neural-Extractive-Summarization-What-Works-and-What’s-Next"><a href="#Searching-for-Effective-Neural-Extractive-Summarization-What-Works-and-What’s-Next" class="headerlink" title="- [ ] Searching for Effective Neural Extractive Summarization: What Works and What’s Next"></a>- [ ] <strong>Searching for Effective Neural Extractive Summarization: What Works and What’s Next</strong></h3><h2 id="AAAI2019论文"><a href="#AAAI2019论文" class="headerlink" title="AAAI2019论文"></a>AAAI2019论文</h2><h3 id="Cycle-SUM-Cycle-consistent-Adversarial-LSTM-Networks-for-Unsupervised-Video-Summarization"><a href="#Cycle-SUM-Cycle-consistent-Adversarial-LSTM-Networks-for-Unsupervised-Video-Summarization" class="headerlink" title="- [ ] Cycle-SUM: Cycle-consistent Adversarial LSTM Networks for Unsupervised Video Summarization"></a>- [ ] Cycle-SUM: Cycle-consistent Adversarial LSTM Networks for <strong>Unsupervised Video Summarization</strong></h3><h3 id="Discriminative-Feature-Learning-for-Unsupervised-Video-Summarization"><a href="#Discriminative-Feature-Learning-for-Unsupervised-Video-Summarization" class="headerlink" title="- [ ] Discriminative Feature Learning for Unsupervised Video Summarization"></a>- [ ] Discriminative Feature Learning for Unsupervised <strong>Video Summarization</strong></h3><h3 id="Abstractive-Text-Summarization-by-Incorporating-Reader-Comments"><a href="#Abstractive-Text-Summarization-by-Incorporating-Reader-Comments" class="headerlink" title="- [ ] Abstractive Text Summarization by Incorporating Reader Comments"></a>- [ ] Abstractive Text Summarization by <strong>Incorporating Reader Comments</strong></h3><h3 id="Exploring-Human-Reading-Cognition-for-Abstractive-Text-Summarization-color-blue-想看"><a href="#Exploring-Human-Reading-Cognition-for-Abstractive-Text-Summarization-color-blue-想看" class="headerlink" title="- [ ] Exploring Human Reading Cognition for Abstractive Text Summarization$\color{blue}{想看}$"></a>- [ ] <strong>Exploring Human Reading Cognition for Abstractive Text Summarization</strong>$\color{blue}{想看}$</h3><h3 id="Generating-Character-Descriptions-for-Automatic-Summarization-of-Fiction"><a href="#Generating-Character-Descriptions-for-Automatic-Summarization-of-Fiction" class="headerlink" title="- [ ] Generating Character Descriptions for Automatic Summarization of Fiction"></a>- [ ] Generating <strong>Character Descriptions</strong> for Automatic Summarization of Fiction</h3><h3 id="Towards-Personalized-Review-Summarization-via-User-aware-Sequence-Network"><a href="#Towards-Personalized-Review-Summarization-via-User-aware-Sequence-Network" class="headerlink" title="- [ ] Towards Personalized Review Summarization via User-aware Sequence Network"></a>- [ ] <strong>Towards Personalized Review Summarization</strong> via User-aware Sequence Network</h3>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/07/18/文本摘要/" data-id="cjy8nqexr0000dkvpiwxu1wlk" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/nlp-summarization/">nlp summarization</a></li></ul>

    </footer>
  </div>
  
</article>


  


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/nlp-summarization/">nlp summarization</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/nlp-summarization/" style="font-size: 10px;">nlp summarization</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/07/18/文本摘要/">文本摘要</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>